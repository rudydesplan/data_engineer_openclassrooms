- # Rapport de gestion de projet ‚Äî Transformation du POC Puls-Events en MVP scalable (RAG + Chatbot)

  ## Introduction

  ### Contexte

  Puls-Events est une plateforme web permettant aux utilisateurs de **d√©couvrir, filtrer et suivre des √©v√©nements culturels en temps r√©el**, collect√©s depuis plusieurs sources (dont **OpenAgenda**) et **personnalis√©s** selon les pr√©f√©rences (lieu, p√©riode, th√©matiques).

  Le **POC** a d√©j√† d√©montr√© la faisabilit√© :

  - d‚Äôun **moteur de recherche s√©mantique** (embeddings + base vectorielle),
  - d‚Äôun **chatbot RAG** (Retrieval-Augmented Generation) capable de recommandations contextualis√©es.

  ### Objectif du projet

  Industrialiser le POC en un **MVP robuste, scalable et observable**, d√©ployable en production, avec priorit√©s explicites :

  1. **M√©moire conversationnelle** (historique + personnalisation durable)
  2. **Contexte g√©ographique optimis√©** (pertinence locale)
  3. **Recherche web en temps r√©el** (outillage/agent)
  4. **Monitoring de performance** (qualit√©, latence, satisfaction)

  Contraintes/choix impos√©s (d√©cision de cadrage) :

  - **AWS** pour l‚Äôinfra principale, avec **Amazon Bedrock** (LLM manag√©) + possibilit√© de mod√®les open-source
  - **Zilliz Cloud** pour la base vectorielle
  - **Chainlit** pour l‚Äôinterface chatbot (MVP)
  - Ajouts recommand√©s : **Guardrails** (fiabilit√©/s√©curit√©), **observabilit√©** (LangSmith/Phoenix), **orchestration** (pipelines RAG modulaires)

  ------

  ## 1. Analyse et synth√®se des besoins formul√©s par l‚Äô√©quipe

  ### 1.1 Synth√®se : contexte, objectifs m√©tiers, contraintes techniques

  #### Objectifs m√©tiers (ce que le MVP doit prouver)

  - **Conversion & engagement** : l‚Äôutilisateur trouve plus vite des √©v√©nements pertinents ‚Üí plus de clics, favoris, suivis.
  - **Diff√©renciation produit** : exp√©rience conversationnelle ‚Äúassistant culturel‚Äù vs moteur de recherche classique.
  - **Scalabilit√© & fiabilit√©** : √©viter ‚Äúd√©mo qui marche‚Äù mais fragile (donn√©es incompl√®tes, hallucinations, latence).
  - **Boucle d‚Äôam√©lioration** : instrumentation permettant d‚Äôit√©rer (feedback, analytics, A/B tests).

  #### Objectifs techniques (ce que le MVP doit garantir)

  - **Pertinence** : bonne s√©lection d‚Äô√©v√©nements (r√©cence, distance, th√®mes, disponibilit√©).
  - **Tra√ßabilit√©** : expliquer ‚Äúpourquoi cette reco ?‚Äù (sources, m√©tadonn√©es, scoring). Les approches RAG index√©es mettent en avant la **tra√ßabilit√© et le retour √† la source** comme b√©n√©fice cl√©.
  - **M√©moire** : passer d‚Äôune conversation isol√©e √† une relation continue. Les syst√®mes de m√©moire d‚Äôagents n√©cessitent des m√©canismes de **stockage + retrieval**, gestion du **contexte limit√©**, et souvent **checkpointing** pour persistance.
  - **S√©curit√©/Conformit√©** : gestion PII, contr√¥le acc√®s, audit.
  - **Observabilit√©** : m√©triques LLM + produit. Bedrock s‚Äôint√®gre √† CloudWatch/CloudTrail et expose des m√©triques (tokens, latence, invocations), + options r√©seau priv√©es (VPC endpoints).

  #### Contraintes et hypoth√®ses (√† expliciter d√®s le MVP)

  - **Donn√©es** : OpenAgenda fournit un export riche (cf. section 4.3 ‚ÄúSch√©ma de donn√©es r√©aliste‚Äù).
  - **Temps r√©el** : ‚Äútemps r√©el‚Äù = ingestion fr√©quente (ex. hourly/near-real-time) + prise en compte ‚Äúderni√®re mise √† jour‚Äù.
  - **Charge** : 3 sc√©narios (faible / moyen / √©lev√©) d√©finis en section co√ªts.
  - **Qualit√© LLM** : non-d√©terministe ‚Üí besoin de guardrails + √©valuation continue.

  ------

  ### 1.2 Analyse des utilisateurs cibles et cas d‚Äôusage

  #### Segments (MVP)

  1. **Explorateur local**
     - ‚ÄúQue faire ce soir pr√®s de moi ?‚Äù
     - Sensible √† distance, horaires, accessibilit√©, prix/inscription.
  2. **Planificateur**
     - ‚ÄúCe week-end : expo + concert, budget limit√©‚Äù
     - Besoin d‚Äôitin√©raire, compatibilit√© calendrier, multi-contraintes.
  3. **Passionn√© th√©matique**
     - ‚ÄúTout ce qui touche au jazz / th√©√¢tre contemporain‚Äù
     - Attentes fortes sur pertinence s√©mantique.
  4. **Touriste**
     - ‚Äú√Ä Paris 3 jours, des √©v√©nements en anglais‚Äù
     - Localisation + langue + p√©riode.

  #### Cas d‚Äôusage prioritaires (MVP)

  - **Recherche conversationnelle** : question libre ‚Üí shortlist d‚Äô√©v√©nements.
  - **Recommandation personnalis√©e** : profil (th√®mes + zones + p√©riodes pr√©f√©r√©es) + historique.
  - **Q/R sur un √©v√©nement** : d√©tails, conditions, accessibilit√©, registration, lien.
  - **Comparaison** : ‚Äúchoisis entre A et B selon mes pr√©f√©rences‚Äù
  - **Web search ‚Äúassist√©e‚Äù** : enrichir un √©v√©nement (ex. infos de derni√®re minute, changement d‚Äô√©tat).

  #### Crit√®res de succ√®s (mesurables)

  S‚Äôinspirer de mesures produit (NPS/CSAT/CES, churn, etc.).

  - m√©triques projet : indicateurs **leading/lagging** et m√©triques ‚ÄúSMART‚Äù (sp√©cifiques, pertinentes, exploitables).

  ------

  ## 2. Plan de projet

  > Approche : **hybride agile** (it√©rations courtes, backlog ordonn√©) + jalons de ‚Äúgo/no-go‚Äù et livrables formels.

  ### 2.1 Jalons (macro)

  | Jalon                             | Objectif                                               | Crit√®re d‚Äôacceptation                |
  | --------------------------------- | ------------------------------------------------------ | ------------------------------------ |
  | M0 ‚Äî Kickoff & cadrage            | scope MVP + risques + architecture cible               | doc de cadrage valid√© + KPI cibles   |
  | M1 ‚Äî Design MVP                   | design d√©taill√© (archi, data, s√©cu, monitoring, co√ªts) | rapport complet + estimation         |
  | M2 ‚Äî Fondations infra             | CI/CD, IaC, environnements dev/stage                   | d√©ploiement automatis√© OK            |
  | M3 ‚Äî Data ingestion v1            | pipeline OpenAgenda ‚Üí stockage + stg                   | donn√©es exploitables + tests         |
  | M4 ‚Äî Vectorisation & retrieval v1 | embeddings + index + filtres                           | retrieval stable + latence cible     |
  | M5 ‚Äî Chat RAG v1 (Chainlit)       | chat fonctionnel + citations sources                   | d√©mos cas d‚Äôusage MVP                |
  | M6 ‚Äî M√©moire + g√©oloc             | personnalisation durable + ranking local               | sc√©narios multi-sessions OK          |
  | M7 ‚Äî Web search tool              | agent/tooling + garde-fous                             | r√©sultats pertinents + rate limiting |
  | M8 ‚Äî Observabilit√© & √©val         | dashboards, traces, evaluation set                     | suivi qualit√© + alerting             |
  | M9 ‚Äî Beta                         | tests utilisateurs, feedback, durcissement             | CSAT/latence > seuils                |
  | M10 ‚Äî Go-Live MVP                 | release contr√¥l√©e                                      | runbook + rollback + monitoring      |

  ### 2.2 √âch√©ancier (exemple 12 semaines)

  | Semaine | Focus                                                    |
  | ------- | -------------------------------------------------------- |
  | S1‚ÄìS2   | M0‚ÄìM1 : design, d√©cisions cloud, backlog, co√ªts, risques |
  | S3      | M2 : IaC + CI/CD + environnements                        |
  | S4‚ÄìS5   | M3 : ingestion + sch√©ma + DQ checks                      |
  | S6      | M4 : embeddings + index + retrieval                      |
  | S7      | M5 : Chainlit + RAG + citations                          |
  | S8      | M6 : m√©moire + g√©oloc + reranking                        |
  | S9      | M7 : web search tool + guardrails                        |
  | S10     | M8 : monitoring + LangSmith/Phoenix + evaluation         |
  | S11     | M9 : beta + feedback + tuning                            |
  | S12     | M10 : go-live + runbooks + handover                      |

  ![Ghaant](C:\Users\Admin\Desktop\DATA\Data Engineer\Master\Openclassrooms\Projet 13\Ghaant.png)

  ### 2.3 Livrables (par domaine)

  R√©f√©rentiel d‚Äôartefacts recommand√© (plans, baselines, backlog, risk register, etc.).

  **Livrables ‚Äúdesign / doc‚Äù**
  
  - Architecture d√©taill√©e (diagrammes + composants + flux)
  - Data model (raw/stg/serving + vector metadata)
  - Strat√©gie m√©moire (ST/LT + consolidation)
  - Strat√©gie g√©olocalisation (PostGIS + filtres)
  - Strat√©gie web search (tooling + garde-fous)
  - Strat√©gie observabilit√© & √©valuation (KPI, dashboards, alerting)
  - Estimation co√ªts build & OPEX + optimisations

  **Livrables ‚Äúcode / infra‚Äù**
  
  - IaC (Terraform/CDK), CI/CD, environnements
  - Pipelines ingestion + vectorisation
  - Service retrieval + RAG orchestrator
  - App Chainlit + API backend
  - Guardrails + tests
  - Monitoring (OpenTelemetry, dashboards)

  ------

  ## 3. Macro backlog des fonctionnalit√©s (format ‚Äútype Excel‚Äù)

  ### 3.1 Principes de backlog & priorisation
  
  - Backlog = enregistrer, suivre, **ordonner** le travail restant ; attention √† ‚Äútout est must‚Äù (pi√®ge MoSCoW) : si tout est critique, la vision est floue.
  - Refinement continu pour garantir des items ‚ÄúReady‚Äù avant dev.
  - D√©composer les epics via **acceptance criteria** et livrer de la valeur √† chaque slice (√©viter d√©coupe purement technique UI/DB).

  ### 3.2 Backlog (extrait structur√© et actionnable)

  **L√©gende**
  
  - Priorit√© : Must / Nice
  - Complexit√© : S (1‚Äì3j), M (4‚Äì7j), L (8‚Äì15j), XL (16j+)
  - Risque : H/M/L
  - Mitigation : action pr√©ventive
  
  | Sprint | Epic          | Feature / User Story                                | Priorit√© | Complexit√© | Estimation | D√©pendances  | Risque | Mitigation                    |
  | ------ | ------------- | --------------------------------------------------- | -------- | ---------- | ---------- | ------------ | ------ | ----------------------------- |
  | 1      | Fondation     | Repo + conventions + quality gates (lint/test)      | Must     | S          | 2j         | ‚Äî            | L      | templates + CI                |
  | 1      | Fondation     | IaC AWS (VPC, IAM, secrets, logs)                   | Must     | M          | 5j         | ‚Äî            | M      | module Terraform + revue s√©cu |
  | 1      | Fondation     | CI/CD (build, test, deploy dev/stage)               | Must     | M          | 5j         | IaC          | M      | environnements isol√©s         |
  | 2      | Data          | Ingestion OpenAgenda (extract)                      | Must     | M          | 6j         | Fondation    | M      | pagination + retries          |
  | 2      | Data          | Stockage raw (S3 + parquet + versioning)            | Must     | M          | 4j         | Ingestion    | L      | conventions partition/date    |
  | 2      | Data          | Normalisation stg (events/venues)                   | Must     | M          | 6j         | raw          | M      | tests + DQ checks             |
  | 2      | Data          | DQ checks (Soda/GreatExp)                           | Must     | M          | 5j         | stg          | M      | seuils + exceptions           |
  | 3      | Vector        | Mod√®le embeddings (choix + wrapper)                 | Must     | M          | 4j         | stg          | M      | abstraction provider          |
  | 3      | Vector        | Indexation Zilliz (collections + partitions)        | Must     | M          | 6j         | embeddings   | M      | POC perf topK                 |
  | 3      | Vector        | Hybrid retrieval (keyword+vector)                   | Must     | M          | 6j         | index        | M      | routing + fusion              |
  | 4      | Retrieval     | Reranking (MMR / r√®gles)                            | Nice     | M          | 5j         | retrieval    | M      | A/B testing                   |
  | 4      | RAG           | Prompt templates + citations + format r√©ponse       | Must     | M          | 5j         | retrieval    | M      | guardrails sortie             |
  | 4      | RAG           | R√©ponses ‚Äútra√ßables‚Äù (source_id, url, dates)        | Must     | S          | 3j         | stg/index    | L      | metadata obligatoire          |
  | 5      | Chat          | App Chainlit (auth l√©g√®re)                          | Must     | M          | 6j         | RAG          | M      | session mgmt                  |
  | 5      | M√©moire       | Short-term memory (session)                         | Must     | M          | 5j         | chat         | M      | Redis TTL                     |
  | 5      | M√©moire       | Long-term memory (profil + pr√©f√©rences)             | Must     | M          | 7j         | auth + db    | H      | minimiser PII + opt-in        |
  | 6      | M√©moire       | Consolidation/summarization                         | Nice     | M          | 6j         | LT memory    | M      | batch jobs                    |
  | 6      | Geo           | G√©oloc user + rayon + ranking distance              | Must     | M          | 6j         | stg venues   | M      | PostGIS + fallback            |
  | 6      | Geo           | Filters multi-crit√®res (dates, tags, accessibilit√©) | Must     | M          | 5j         | stg          | L      | index + query plan            |
  | 7      | Web           | Tool ‚Äúweb_search‚Äù via smolagents                    | Nice     | L          | 10j        | guardrails   | H      | allowlist + cache             |
  | 7      | Guardrails    | Output schema (Pydantic/JSON)                       | Must     | M          | 6j         | RAG          | M      | Guardrails/Instructor         |
  | 8      | Guardrails    | Policy & safety (NeMo)                              | Nice     | L          | 10j        | chat         | M      | r√®gles minimales              |
  | 8      | Observabilit√© | Tracing LLM (LangSmith)                             | Must     | S          | 3j         | RAG          | L      | sampling                      |
  | 8      | Observabilit√© | Phoenix eval dashboard + dataset                    | Nice     | M          | 7j         | traces       | M      | gold set                      |
  | 9      | Produit       | Feedback utilisateur (üëç/üëé + raisons)                | Must     | S          | 3j         | chat         | L      | stockage events               |
  | 9      | Produit       | CSAT/NPS/CES mini-sondage                           | Nice     | S          | 3j         | feedback     | L      | d√©clenchement                 |
  | 9      | Ops           | Alerting latence/taux erreurs                       | Must     | S          | 3j         | logs/metrics | L      | seuils + runbook              |
  | 10     | S√©curit√©      | IAM least privilege + audit CloudTrail              | Must     | M          | 5j         | IaC          | M      | revue + tests                 |
  | 10     | Release       | Beta ferm√©e + it√©rations                            | Must     | M          | 7j         | tout         | M      | crit√®res d‚Äôentr√©e/sortie      |

  ------

  ## 4. Architecture technique d√©taill√©e

  ### 4.1 Sch√©ma d‚Äôarchitecture cloud scalable (AWS + Bedrock + Zilliz + Chainlit)

  #### Vue d‚Äôensemble (cible MVP)

  ![image-20260213124417067](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20260213124417067.png)

  

  

  
  
  
  
  

  

  
  
  #### Composants AWS recommand√©s (MVP pragmatique)
  
  **Compute**
  
  - **ECS Fargate** (containers) pour :
    - `api-backend` (FastAPI)
    - `rag-orchestrator` (peut √™tre dans le backend au d√©but, mais modularisable)
    - `ingestion-worker` (jobs planifi√©s)
  - Alternative : Lambda pour ingestion l√©g√®re, mais ingestion + ETL + embeddings peuvent d√©passer (dur√©e/m√©moire) ‚Üí Fargate plus simple.
  
  **R√©seau**
  
  - VPC + subnets priv√©s
  - **VPC Endpoint / PrivateLink** pour Bedrock : trafic priv√©, pas d‚Äôinternet public pour les appels mod√®le.
  
  **Stockage & bases**
  
  - **S3** : raw exports + snapshots + artefacts d‚Äô√©valuation
  - **RDS PostgreSQL + PostGIS** :
    - tables `stg.events`, `stg.venues`, `user_profiles`, `user_feedback`
    - g√©o-queries (rayon, bounding box, index GiST)
  - **Zilliz Cloud** : vecteurs (event embeddings + √©ventuellement memory embeddings)
  - **Redis (ElastiCache)** : session memory (short-term), caching (semantic cache option)
    - la **semantic caching** est cit√©e comme approche co√ªt/perf, avec limites en multi-tours.
  
  **LLM**
  
  - **Option A (MVP rapide)** : **Amazon Bedrock** (LLM manag√©)
    - b√©n√©fices : gouvernance, m√©triques, IAM, CloudTrail/CloudWatch.
  - **Option B (open-source Llama 3.1 70B)** : self-hosting (SageMaker / EKS / EC2 GPU)
    - b√©n√©fices : contr√¥le co√ªt par token (selon usage), souverainet√© mod√®le
    - co√ªts/complexit√© sup√©rieurs (GPU, scaling, MLOps) ‚Üí option ‚Äúphase 2‚Äù sauf exigence forte.
  
  **Observabilit√©**
  
  - CloudWatch (logs/metrics)
  - OpenTelemetry (traces)
  - **LangSmith** : tracing agents/chains
  - **Arize Phoenix** : eval + observabilit√© LLM (offline/online)
  
  ------
  
  ### 4.2 Choix du cloud provider & veille technique (justification)
  
  #### Pourquoi AWS (et pas GCP/Azure) ‚Äî crit√®res MVP
  
  1. **Bedrock** : service manag√© LLM avec gouvernance (IAM), audit (CloudTrail), m√©triques (CloudWatch) et options r√©seau priv√©es (VPC endpoints).
  2. **√âcosyst√®me complet** : une app GenAI est plus qu‚Äôun mod√®le, elle requiert plusieurs composants (data, s√©curit√©, d√©ploiement, etc.).
  3. **Scalabilit√© standard** : ECS, RDS, S3, observabilit√© native.
  4. **Conformit√©** : choix de r√©gion, chiffrement, gestion cl√©s.

  #### Veille sur options march√© (vector DB / UI / tracing)

  - **Vector DB** : Zilliz Cloud (impos√©), alternatives : Pinecone, Weaviate, pgvector (si on voulait tout dans Postgres).
  - **UI Chat** : Chainlit (impos√© MVP), alternatives : Streamlit, Next.js.
  - **Tracing/Eval** : LangSmith, Phoenix, OpenTelemetry.

  ------
  
  ### 4.3 Sch√©ma de donn√©es ( OpenAgenda)
  
  #### 4.3.1 Champs OpenAgenda (source)

  Le fichier OpenAgenda contient notamment : Identifiant, Slug, URL canonique, Titre, Description, Description longue, Mots cl√©s, Image, Derni√®re mise √† jour, R√©sum√© horaires, dates d√©but/fin, horaires d√©taill√©s, accessibilit√©, identifiant lieu, coordonn√©es, adresse, ville, r√©gion, pays, tags lieu, √©tat √©v√©nement, √¢ge min/max, cat√©gorie, registration, liens additionnels, etc.

  #### 4.3.2 Mod√®le de donn√©es propos√© (3 couches)
  
  **But** : s√©parer brut / normalis√© / serving + garantir tra√ßabilit√©.
  
  ##### A) RAW (S3 + √©ventuellement Postgres raw)
  
  - `raw.openagenda_events` (append-only, versionn√©)
    - colonnes ‚Äútelles quelles‚Äù
    - `ingested_at`, `source_file`, `hash_row`
  - `raw.openagenda_venues`
    - d√©duit de ‚ÄúIdentifiant du lieu‚Äù (d√©doublonnage)
  
  ##### B) STAGING (Postgres)
  
  - `stg.events`
    - `event_id` (Identifiant)
    - `slug`, `canonical_url`
    - `title`, `description_short`, `description_long`
    - `keywords`, `category`
    - `image_url`, `image_credits`
    - `updated_at` (Derni√®re mise √† jour)
    - `schedule_summary`, `schedule_detailed`
    - `date_first_start`, `date_first_end`, `date_last_start`, `date_last_end`
    - `accessibility_code`, `accessibility_text`
    - `is_online`, `online_url`
    - `event_status`
    - `age_min`, `age_max`
    - `registration`
    - `additional_links` (JSONB)
    - `venue_id`
    - `source_agenda_title`, `source_agenda_uid`
    - `contributor_email` (‚ö†Ô∏è PII), `contributor_phone`, `contributor_name`, etc. (souvent √† exclure/masquer)
  - `stg.venues`
    - `venue_id` (Identifiant du lieu)
    - `name`, `address`, `zipcode`, `city`, `region`, `country`
    - `geo_point` (PostGIS geography POINT)
    - `venue_phone`, `venue_website`, `venue_links` (JSONB)
    - `venue_tags`, `venue_description`, `access_route`
    - `venue_image_url`, `venue_image_credits`
  - `stg.event_tags` (si besoin de normaliser tags)
  - `stg.event_occurrences` (optionnel, si horaires d√©taill√©s explos√©s)
  
  ##### C) SERVING (Postgres)
  
  - `app.user_profile`
    - pr√©f√©rences : villes/zones, th√®mes, horaires, contraintes (budget, accessibilit√©)
  - `app.user_events`
    - favoris, suivis, clics
  - `app.feedback`
    - thumbs up/down + raisons + ‚Äúr√©ponse utile ?‚Äù
  - `ops.rag_queries`
    - logs structur√©s : query, top_k ids, latence, score, mod√®le, tokens (pour co√ªts)
  
  #### 4.3.3 Vector store (Zilliz)
  
  - Collection `events_embeddings`
    - `event_id` (PK)
    - `embedding` (vector)
    - `metadata` (JSON) : dates, category, city, geo_hash, is_online, status, updated_at
    - champs pour filtrage rapide (selon capacit√©s Zilliz)
  - Option (phase 2) : collection `memories_embeddings` (pour m√©moire long terme)
  
  ------
  
  ### 4.4 Design RAG & Agents (scalable + fiable)
  
  #### 4.4.1 Pipeline RAG modulaire (recommand√©)
  
  Un pipeline RAG se structure en composants/pipelines ind√©pendants (collecte ‚Üí pr√©paration ‚Üí embeddings ‚Üí retrieval ‚Üí g√©n√©ration), ce qui facilite le travail parall√®le et la mont√©e en charge.
  
  **Pipelines propos√©s**
  
  1. **Pipeline A ‚Äî Data ingestion**
     - extraction OpenAgenda + normalisation + DQ checks
  2. **Pipeline B ‚Äî Embeddings & index**
     - chunking (si n√©cessaire) + embeddings + upsert Zilliz
  3. **Pipeline C ‚Äî Retrieval & ranking**
     - query routing + hybrid search + filtres (dates/geo)
  4. **Pipeline D ‚Äî Generation**
     - prompt augmentation + citations + garde-fous
  5. **Pipeline E ‚Äî Evaluation**
     - dataset de tests + m√©triques + feedback loop
  
  #### 4.4.2 Retrieval avanc√© : hybrid + routing + reranking
  
  - **Query routing** : d√©cider ‚Äúkeyword vs semantic vs hybrid‚Äù selon le contenu de la requ√™te.
  - **Hybrid search** : combiner keyword match (noms, lieux) et semantic (intent).
  - **Indexing & traceability** : les approches index√©es augmentent vitesse, pr√©cision et tra√ßabilit√© √† la source.
  
  #### 4.4.3 M√©moire conversationnelle (design)
  
  Les syst√®mes convergent vers 3 formes long-terme : **√©pisodique**, **s√©mantique**, **proc√©durale**.
  Et il faut g√©rer la contrainte : le contexte du mod√®le est limit√© et co√ªteux ‚Üí on externalise via RAG/m√©moire.

  **Proposition MVP (simple mais ‚Äúproduction-minded‚Äù)**

  - **Short-Term Memory (STM)** : Redis
    - contenu : derniers tours + variables session (ville courante, dates, th√®mes)
    - TTL (ex. 24h), purge automatique
  - **Long-Term Memory (LTM)** : Postgres + (option) Zilliz
    - **profil s√©mantique** : pr√©f√©rences stables (tags, villes)
    - **√©pisodique** : ‚Äúdernier weekend tu as aim√© X‚Äù
    - consolidation : r√©sum√©s p√©riodiques
  - **Checkpointing** : snapshots de session (r√©sum√© + slots) pour reprise multi-sessions.
  - **Semantic caching** (Nice-to-have) : acc√©l√©rer questions fr√©quentes (mais attention multi-tours).

  #### 4.4.4 Contexte g√©ographique optimis√©

  **Objectif** : ranking local ‚Äúpar d√©faut‚Äù, sans que l‚Äôutilisateur r√©p√®te ‚Äúpr√®s de moi‚Äù.

  - Stocker `user_location` (opt-in) et/ou ville pr√©f√©r√©e
  - Calcul :
    - candidates via filtres (dates, status)
    - scoring final = pertinence s√©mantique + bonus distance + bonus fra√Æcheur
  - Impl√©mentation :
    - PostGIS pour calcul distance et filtrage radius
    - Zilliz pour retrieval s√©mantique ; on intersecte avec r√©sultats PostGIS, ou on utilise metadata filters si suffisant.

  ------

  ### 4.5 Strat√©gies de d√©ploiement, modularit√©, monitoring
  
  #### 4.5.1 D√©ploiement (MVP ‚Üí scalable)
  
  - **IaC** : Terraform (modules : r√©seau, ECS, RDS, Redis, IAM)
  - **CI/CD** : GitHub Actions (build/test ‚Üí deploy dev ‚Üí approval ‚Üí stage/prod)
  - **Environnements** : dev / stage / prod
  - **Secrets** : AWS Secrets Manager
  - **Release** : blue/green ou rolling ECS
  
  #### 4.5.2 Modularit√© (√©viter monolithe )
  
  D√©couper par capacit√©s :
  
  - `ingestion-service`
  - `embedding-service`
  - `retrieval-service`
  - `rag-orchestrator`
  - `chat-ui` (Chainlit)
  - `observability` (collectors, dashboards)
  
  > B√©n√©fice : on peut scaler embedding ind√©pendamment du chat.

  #### 4.5.3 Monitoring (technique + produit)

  **Niveau infra**
  
  - latence API, erreurs, CPU/mem
  - RDS connections, slow queries
  - Redis hit rate

  **Niveau LLM/RAG**

  - tokens in/out, latence d‚Äôinvocation Bedrock, invocations (CloudWatch).
  - retrieval latency, topK size, rerank time
  - taux r√©ponses avec citations

  **Niveau produit**
  
  - adoption : utilisateurs actifs, sessions, r√©tention
  - satisfaction : CSAT/NPS/CES (si activ√©).
  - qualit√© : thumbs up/down, ‚Äúcomplaint-like prompts‚Äù
  
  **Seuils & exception plans**
  
  - d√©finir des tol√©rances (latence, budget, erreurs) + plan d‚Äôaction si d√©passement.
  
  ------
  
  ## 5. Estimation des co√ªts Build & OPEX (format ‚Äútype Excel‚Äù)
  
  > ‚ö†Ô∏è Les prix cloud changent fr√©quemment. Le MVP doit livrer **un mod√®le de co√ªts** + sc√©narios, et valider les chiffres via AWS Pricing Calculator + offres Zilliz au moment du d√©ploiement.
  > Ci-dessous : structure exhaustive + drivers + estimations relatives.
  
  | Poste                                    | Jours | TJM (indicatif) | Sous-total | Notes              |
  | ---------------------------------------- | ----- | --------------- | ---------- | ------------------ |
  | Design MVP (archi + data + s√©cu + costs) | 10    | 750 ‚Ç¨           | 7 500 ‚Ç¨    | livrable principal |
  | IaC + CI/CD                              | 8     | 800 ‚Ç¨           | 6 400 ‚Ç¨    | base production    |
  | Ingestion + normalisation + DQ           | 12    | 750 ‚Ç¨           | 9 000 ‚Ç¨    | robustesse data    |
  | Embeddings + Zilliz + retrieval          | 12    | 750 ‚Ç¨           | 9 000 ‚Ç¨    | perf + filtres     |
  | RAG + Chainlit + guardrails              | 12    | 750 ‚Ç¨           | 9 000 ‚Ç¨    | fiabilit√© outputs  |
  | M√©moire + g√©oloc                         | 10    | 750 ‚Ç¨           | 7 500 ‚Ç¨    | personnalisation   |
  | Web search tool                          | 6     | 750 ‚Ç¨           | 4 500 ‚Ç¨    | Nice-to-have       |
  | Observabilit√© + eval + dashboards        | 8     | 775 ‚Ç¨           | 6 200 ‚Ç¨    | pilotage qualit√©   |
  | Beta + hardening + runbooks              | 8     | 775 ‚Ç¨           | 6 200 ‚Ç¨    | go-live            |

  **Total charge** : ~86 jours (√† ajuster selon p√©rim√®tre Nice-to-have)

  ### Total Build (hors marge)

  **65 300 ‚Ç¨**

  ### Provision recommand√©e (risques / impr√©vus)

  - **+15%** (tickets non anticip√©s, ajustements perf, s√©curit√©, feedback utilisateurs) : **9 795 ‚Ç¨**
  
  ‚úÖ **Total Build (avec provision 15%) : 75 095 ‚Ç¨**

  ------

  ### 5.2 Co√ªt d‚Äôexploitation (OPEX) selon charges pr√©vues

  ### 5.2.0 Principes de chiffrage (comment on √©vite les ‚Äúsurprises‚Äù)

  L‚ÄôOPEX du MVP d√©pend principalement de **l‚Äôusage** (requ√™tes, tokens, volume d‚Äôindexation, trafic) et de **la capacit√© minimale** √† maintenir (API, workers, BDD relationnelle, cache, logs/monitoring). 
  
  L‚Äôapproche la plus fiable consiste √† :
  
  1. **d√©composer par ‚Äúcost drivers‚Äù** (tokens, vCPU-seconde, GB-mois, m√©triques, etc.),
  2. **d√©finir 3 sc√©narios de charge** (MVP ‚Üí croissance ‚Üí scale),
  3. **estimer via l‚ÄôAWS Pricing Calculator** + un tableau ‚ÄúExcel‚Äù interne qui suit les hypoth√®ses.
  
  > √Ä noter : Bedrock fonctionne en ‚Äúpay-per-use‚Äù (inf√©rence) et propose aussi des options de capacit√© r√©serv√©e/tiers ; les prix varient par **r√©gion** et **mod√®le**. ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))
  
  
  
  #### Variables cl√©s (cost drivers)
  
  1. **LLM inference** (tokens in/out) ‚Äî principal poste si forte adoption
  2. **Vector DB (Zilliz)** ‚Äî stockage + requ√™tes/s
  3. **Compute ECS** ‚Äî API + workers
  4. **RDS Postgres** ‚Äî taille + IOPS
  5. **Redis** ‚Äî m√©moire + d√©bit
  6. **Observabilit√©** ‚Äî logs volumineux + traces

  #### Sc√©narios de charge (exemple)

  | Sc√©nario    | Utilisateurs/jour | Requ√™tes chat/jour | Tokens moyens / req | QPS pic |
  | ----------- | ----------------- | ------------------ | ------------------- | ------- |
  | S1 (faible) | 200               | 1 000              | 1 500               | 2       |
  | S2 (moyen)  | 2 000             | 15 000             | 2 000               | 10      |
  | S3 (√©lev√©)  | 20 000            | 150 000            | 2 500               | 50      |
  
  ------
  
  ### 5.2.1 Outil de r√©f√©rence : AWS Pricing Calculator (et comment l‚Äôutiliser)
  
  AWS recommande l‚ÄôAWS Pricing Calculator pour construire un chiffrage mensuel par service, par r√©gion, et exporter en CSV/PDF. Le guide pr√©cise aussi qu‚Äôil n‚Äôy a **pas d‚ÄôAPI** officielle du calculator (donc on versionne nos hypoth√®ses c√¥t√© projet). ([AWS Documentation](https://docs.aws.amazon.com/pricing-calculator/latest/userguide/getting-started.html))
  
  **Pages cl√©s √† pr√©senter dans le rapport (c‚Äôest exactement le workflow demand√©)** :
  
  - **Landing** ‚Üí cr√©ation d‚Äôestimation
  - **Add service** (page ‚Äúajouter un service‚Äù)
  - **Configure service** (r√©gion + param√®tres)
  - **My estimate** (r√©cap + export CSV/PDF) ([AWS Documentation](https://docs.aws.amazon.com/pricing-calculator/latest/userguide/getting-started.html))
  
  **Bonnes pratiques (√† inclure dans le plan FinOps du MVP)**

  - 1 estimate par **environnement** (dev / staging / prod).
  - 1 estimate par **sc√©nario de charge** (S1/S2/S3).
  - Groupes par ‚Äúcost centers‚Äù (LLM / Data / Observabilit√© / R√©seau). ([AWS Documentation](https://docs.aws.amazon.com/pricing-calculator/latest/userguide/getting-started.html))

  ------
  
  ### 5.2.2 Zilliz Cloud (Milvus manag√©) ‚Äî options, mod√®le de co√ªt, et Marketplace AWS

  #### Option A ‚Äî Zilliz Cloud via site Zilliz (pricing)

  - **Free / Free tier** : 5 GB stockage + 2.5M vCUs / mois (utile dev/POC, tr√®s limit√© pour prod). ([docs.zilliz.com](https://docs.zilliz.com/docs/free-trials?utm_source=chatgpt.com))
  - **Standard** : √† partir de **$0/mois (Serverless)** et **$99/GB/mois (Dedicated)** (positionnement ‚ÄúMVP ‚Üí prod‚Äù). ([zilliz.com](https://zilliz.com/pricing?utm_source=chatgpt.com))
  - **Serverless** : pay-as-you-go sur consommation (vCUs) + stockage si applicable ; logique ‚Äútu ne payes presque rien quand c‚Äôest inactif, hors stockage‚Äù. ([docs.zilliz.com](https://docs.zilliz.com/docs/view-invoice?utm_source=chatgpt.com))

  #### Option B ‚Äî ‚ÄúSubscribe on Marketplace (AWS)‚Äù (recommand√© pour MVP en entreprise)

  Avantages : **facturation centralis√©e AWS**, facilit√© procurement, et consolidation FinOps.

  - La fiche AWS Marketplace indique : abonnement sans date de fin, annulable √† tout moment, facturation √† l‚Äôusage. ([Amazon Web Services, Inc.](https://aws.amazon.com/marketplace/pp/prodview-iqbidum7feuio))
  - **Dimension de facturation Marketplace** : ‚ÄúZilliz Cloud Usage‚Äù ‚Üí **$0.001 / unit**, et **1 unit = 0.1 cent** (donc 1$ = 1000 units). ([Amazon Web Services, Inc.](https://aws.amazon.com/marketplace/pp/prodview-iqbidum7feuio))
  - Mention importante : ‚ÄúAdditional AWS infrastructure costs may apply‚Äù ‚Üí on chiffre tout avec AWS Pricing Calculator + hypoth√®ses Zilliz. ([Amazon Web Services, Inc.](https://aws.amazon.com/marketplace/pp/prodview-iqbidum7feuio))

  **Recommandation MVP (co√ªt/risque)**

  - **MVP early** : Serverless (si charge tr√®s variable)
  - **MVP stable + SLA** : Dedicated (latence/throughput plus pr√©dictibles)
  - **Scale / contraintes conformit√©** : BYOC (quand on veut isoler infra & r√©seau)
  
  ------

  ### 5.2.3 Amazon Bedrock ‚Äî ce qu‚Äôon paie r√©ellement (tokens, tiers, batch, provisioned)

  #### A) Inference On-demand (tokens)

  Bedrock facture l‚Äôinf√©rence en fonction du **volume de tokens input/output** (d√©pend du mod√®le/provider), et propose plusieurs **tiers** (Standard/Flex/Priority/Reserved). ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))

  **Exemple de pricing officiel (Anthropic Claude 3.5 Sonnet ‚Äì effective 1 Dec 2025)**
  
  - **$6.00 / 1M input tokens**
  - **$3.00 / 1M output tokens** ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))

  ‚û°Ô∏è **Formule ‚Äúunit economics‚Äù** (√† mettre dans votre table OPEX) :
  `Co√ªt_LLM = (InputTokens/1,000,000 * PrixIn) + (OutputTokens/1,000,000 * PrixOut)` ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))
  
  #### B) Batch inference (quand possible)
  
  Bedrock indique que certains mod√®les ont un **prix batch ~50% moins cher** que l‚Äôon-demand (utile pour traitements non temps r√©el : r√©-indexation, enrichissements, r√©sum√©s offline). ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))
  
  #### C) Provisioned / Reserved / Custom Model Import (cas ‚ÄúLLama 3.1 70B‚Äù & capacit√©)
  
  Pour des besoins de capacit√©/latence garantis, on peut passer par des options de capacit√© (selon mod√®le). Bedrock documente aussi le **Custom Model Import** avec un dimensionnement en ‚ÄúCustom Model Units‚Äù.
  Ex : r√©f√©rence de capacit√© : **Llama 3.1 70B 128k n√©cessite 8 Custom Model Units**. ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))

  ------
  
  ### 5.2.4 Compute API & services (ECS Fargate) ‚Äî co√ªt, options, leviers

  #### Ce qu‚Äôon paie

  Fargate est factur√© sur :
  
  - vCPU, m√©moire, stockage √©ph√©m√®re, OS/arch, dur√©e (arrondie √† la seconde) ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  - **20 GB de stockage √©ph√©m√®re inclus** ; on paie seulement l‚Äôextra. ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  - dur√©e : **1-minute minimum** (Windows : 5 minutes min). ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  
  **Exemples chiffr√©s officiels (US East, Linux/x86)** :
  
  - CPU : **$0.000011244 / vCPU-second**
  - RAM : **$0.000001235 / GB-second**
  - Storage extra : **$0.0000000308 / GB-second** ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  
  #### Options pour r√©duire l‚ÄôOPEX
  
  - **Fargate Spot** : jusqu‚Äô√† **-70%** (workloads interruptibles : indexing, batch jobs). ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  - **Compute Savings Plans** : jusqu‚Äô√† **-50%** pour usage stable. ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  
  ------
  
  ### 5.2.5 Base SQL (RDS PostgreSQL) ‚Äî composants de co√ªt √† mod√©liser
  
  Pour RDS PostgreSQL, les principaux postes sont :
  
  - **DB instance hours** (facturation √† la seconde, minimum 10 minutes)
  - **stockage (GB-mois)**
  - **IO requests / provisioned IOPS** (selon stockage)
  - **backup storage**
  - **data transfer**
  - option : **Reserved Instances** pour r√©duire la facture si usage stable ([AWS Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/User_DBInstanceBilling.html))

  RDS propose plusieurs types de stockage (ex. gp2/gp3/io1) avec impacts sur co√ªts/perfs. ([AWS Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html))
  
  **Point d‚Äôattention** (souvent oubli√© en MVP) : si vous utilisez des instances ‚Äúburstable‚Äù (T3/T4g) en mode unlimited, il existe un co√ªt de **CPU credits**. Exemple cit√© : **$0.075 / vCPU-hour** pour les CPU credits. ([Amazon Web Services, Inc.](https://aws.amazon.com/rds/postgresql/pricing/))
  
  ------
  
  ### 5.2.6 Cache (ElastiCache Redis) ‚Äî Serverless vs nodes
  
  ElastiCache a deux approches courantes :
  
  #### A) ElastiCache Serverless
  
  Facturation bas√©e sur :
  
  - **donn√©es stock√©es (GB-hours)**
  - **ECPUs consomm√©es** (par volume trait√©) ([Amazon Web Services, Inc.](https://aws.amazon.com/elasticache/pricing/))
  
  #### B) ElastiCache ‚Äúnode-based‚Äù
  
  - facturation ‚Äúnode-hours‚Äù, et (selon la doc) **arrondie √† l‚Äôheure compl√®te** ([Amazon Web Services, Inc.](https://aws.amazon.com/elasticache/pricing/))
  
  Backups : exemple de tarif indiqu√© : **$0.085 / GiB-month**. ([Amazon Web Services, Inc.](https://aws.amazon.com/elasticache/pricing/))
  
  ------

  ### 5.2.7 Stockage & data lake (S3) ‚Äî ce qu‚Äôil faut chiffrer

  S3 se chiffre via :

  - **stockage (GB-mois)** selon classe (Standard, IA, Glacier, etc.)
  - **requests** (PUT/GET/LIST)
  - **data transfer** (intra/inter-region, internet egress)
  - **features** (ex. Transfer Acceleration) ([Amazon Web Services, Inc.](https://aws.amazon.com/s3/pricing/))
  
  Exemples de tarifs mentionn√©s sur S3 pricing (features r√©seau) :

  - Transfer Acceleration : **$0.04‚Äì$0.08 / GB** selon zone
  - Data routing : **$0.002‚Äì$0.004 / GB** selon zone ([Amazon Web Services, Inc.](https://aws.amazon.com/s3/pricing/))
  
  ------
  
  ### 5.2.8 Observabilit√© (CloudWatch) ‚Äî m√©triques, logs, alarmes, dashboards
  
  CloudWatch se chiffre typiquement sur :
  
  - **logs ing√©r√©s + stock√©s**
  - **custom metrics**
  - **alarmes**
  - **dashboards** ([AWS Documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_billing.html))
  
  Exemples chiffr√©s (extraits pricing) :
  
  - **Composite alarm** : **$0.50 / month** (pricing listing) ([Amazon Web Services, Inc.](https://aws.amazon.com/cloudwatch/pricing/))
  - **Alarm metric (standard resolution)** : **$0.10 / alarm metric** (pricing listing) ([Amazon Web Services, Inc.](https://aws.amazon.com/cloudwatch/pricing/))
  - **Custom metrics** : exemple affich√© **$0.30 per metric-month** (selon paliers) ([Amazon Web Services, Inc.](https://aws.amazon.com/cloudwatch/pricing/))
  
  ------
  
  ## 5.2.9 Tableau ‚Äútype Excel‚Äù ‚Äî Drivers OPEX √† suivre (unit√©s + prix + options)

  > Copiable tel quel dans Excel (√† compl√©ter avec vos hypoth√®ses S1/S2/S3).

  | Poste                              | Unit√© de co√ªt                                      | Exemple de prix (r√©f√©rence)                                  | Options / remarques FinOps                                   |
  | ---------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | **Bedrock (LLM inference)**        | 1M input/output tokens                             | Claude 3.5 Sonnet : $6 / 1M input ; $3 / 1M output           | Tiers (Standard/Flex/Priority/Reserved) + Batch possible (-50% sur mod√®les √©ligibles) ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/)) |
  | **Bedrock (Custom Model Import)**  | Custom Model Unit (dimensionnement)                | Llama 3.1 70B 128k = 8 CMU (dimensionnement)                 | utile si vous importez/servez un mod√®le ; facturation en fen√™tres (voir page pricing) ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/)) |
  | **Zilliz Cloud (site)**            | GB/mois (Dedicated)                                | From $99/GB/month                                            | Free tier 5GB+2.5M vCU ; Serverless PAYG ; Dedicated plus pr√©dictible ([zilliz.com](https://zilliz.com/pricing?utm_source=chatgpt.com)) |
  | **Zilliz Cloud (AWS Marketplace)** | ‚ÄúUsage unit‚Äù                                       | $0.001 / unit (1 unit=0.1 cent)                              | Abonnement annulable ; facturation AWS ; ‚Äúadditional infra costs may apply‚Äù ([Amazon Web Services, Inc.](https://aws.amazon.com/marketplace/pp/prodview-iqbidum7feuio)) |
  | **ECS Fargate**                    | vCPU-second / GB-second                            | $0.000011244 / vCPU-s ; $0.000001235 / GB-s (US East example) | Spot jusqu‚Äô√† -70% ; Savings Plans jusqu‚Äô√† -50% ; 20GB ephemeral inclus ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/)) |
  | **RDS PostgreSQL**                 | instance-hours + storage GB-mois + I/O + backups   | (d√©pend instance/r√©gion)                                     | billing √† la seconde (min 10 min) + Reserved Instances possibles ([AWS Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/User_DBInstanceBilling.html)) |
  | **ElastiCache Redis**              | GB-hours + ECPUs (serverless) / node-hours (nodes) | (d√©pend r√©gion)                                              | backups : $0.085/GiB-month (exemple) ([Amazon Web Services, Inc.](https://aws.amazon.com/elasticache/pricing/)) |
  | **S3**                             | GB-mois + requests + transfer                      | Transfer Acceleration $0.04‚Äì$0.08/GB (ex)                    | choisir classes (Standard/IA/Glacier) selon acc√®s ; surveiller egress ([Amazon Web Services, Inc.](https://aws.amazon.com/s3/pricing/)) |
  | **CloudWatch**                     | logs + metrics + alarms                            | composite alarm $0.50/mo ; alarm metric $0.10                | logs + m√©triques explosent vite : quotas + sampling + r√©tention courte ([Amazon Web Services, Inc.](https://aws.amazon.com/cloudwatch/pricing/)) |

  ------

  

  Voici un **tableau chiffr√© mensuel concret (USD)** bas√© sur :

  - Pricing Bedrock exemple Claude 3.5 Sonnet
    ‚Üí **$6 / 1M input tokens**
    ‚Üí **$3 / 1M output tokens**
  - R√©partition moyenne : **60% input / 40% output**
  - 30 jours par mois
  - Fargate Linux/x86 pricing officiel
  - Estimations r√©alistes pour RDS / Redis / Zilliz / CloudWatch
  
  ‚ö†Ô∏è Hypoth√®ses coh√©rentes mais indicatives (√† affiner avec AWS Calculator).
  
  ------
  
  # üìä ESTIMATION OPEX MENSUELLE ‚Äì CONFIGURATION COMPL√àTE
  
  ## üîπ Hypoth√®ses de base
  
  | Sc√©nario | Req / jour | Tokens / req | Tokens / mois |
  | -------- | ---------- | ------------ | ------------- |
  | S1       | 1 000      | 1 500        | 45 M          |
  | S2       | 15 000     | 2 000        | 900 M         |
  | S3       | 150 000    | 2 500        | 11,25 B       |
  
  R√©partition tokens :
  
  - 60% input
  - 40% output

  ------
  
  # üìà TABLEAU EXCEL ‚Äì OPEX MENSUEL PAR SC√âNARIO
  
  | Poste                           | S1 (faible) | S2 (moyen)  | S3 (√©lev√©)   | Hypoth√®ses                       |
  | ------------------------------- | ----------- | ----------- | ------------ | -------------------------------- |
  | **Bedrock (LLM)**               | **216 $**   | **4 320 $** | **54 000 $** | 60% input @ $6 / 40% output @ $3 |
  | **ECS Fargate (API + workers)** | 36 $        | 213 $       | 1 422 $      | Autoscaling 2 ‚Üí 20 tasks         |
  | **RDS PostgreSQL**              | 150 $       | 400 $       | 1 200 $      | Instance + storage + backups     |
  | **Redis (ElastiCache)**         | 50 $        | 150 $       | 400 $        | Cache + sessions                 |
  | **Zilliz Cloud**                | 200 $       | 800 $       | 2 500 $      | Serverless ‚Üí Dedicated scale     |
  | **S3 (stockage + requ√™tes)**    | 20 $        | 80 $        | 300 $        | Raw + logs + backups             |
  | **CloudWatch (logs + metrics)** | 50 $        | 200 $       | 600 $        | Sampling progressif              |

  ------

  # üí∞ TOTAL OPEX MENSUEL ESTIM√â

  | Sc√©nario | Total mensuel       |
  | -------- | ------------------- |
  | **S1**   | **722 $ / mois**    |
  | **S2**   | **6 163 $ / mois**  |
  | **S3**   | **60 422 $ / mois** |

  ------

  # üîç Lecture strat√©gique
  
  ### üü¢ S1 ‚Äì MVP Early Stage
  
  ‚âà **700‚Äì1 000 $ / mois**
  
  - Tr√®s soutenable
  - LLM repr√©sente ~30% du co√ªt
  - Infrastructure marginale
  
  üëâ Id√©al pour phase b√™ta.
  
  ------
  
  ### üü° S2 ‚Äì Croissance ma√Ætris√©e
  
  ‚âà **6 000 $ / mois**
  
  - LLM devient poste dominant
  - N√©cessit√© de :
    - routing multi-mod√®le
    - optimisation tokens
    - caching
  
  ------
  
  ### üî¥ S3 ‚Äì Scale important
  
  ‚âà **60 000 $ / mois**
  
  - **LLM = 90% du co√ªt**
  - Obligatoire :
    - multi-mod√®le
    - compression contexte
    - r√©duction top-k
    - √©ventuellement self-host Llama 70B
  
  ------
  
  # Ce que ce tableau d√©montre dans le rapport
  
  1. Le MVP est √©conomiquement viable en phase initiale
  2. Le vrai facteur d‚Äôexplosion budg√©taire = tokens
  3. L‚Äôarchitecture doit √™tre pens√©e ‚Äúcost-aware‚Äù
  4. Les optimisations budg√©taires ne sont pas optionnelles √† scale
  
  ------
  
  ## 5.2.10 ‚ÄúMini-mode d‚Äôemploi‚Äù : quoi renseigner dans l‚ÄôAWS Pricing Calculator (par service)
  
  - **ECS Fargate** : nb services/tasks, vCPU/RAM, dur√©e (24/7 ou bursts), storage, trafic sortant. ([Amazon Web Services, Inc.](https://aws.amazon.com/fargate/pricing/))
  - **RDS PostgreSQL** : type d‚Äôinstance, Multi-AZ, stockage, IOPS, backups, data transfer. ([AWS Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/User_DBInstanceBilling.html))
  - **S3** : volume stock√© par classe, nb requ√™tes GET/PUT/LIST, data transfer. ([AWS Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html))
  - **CloudWatch** : logs ingest√©s/jour, r√©tention, nb custom metrics, nb alarmes/dashboards. ([AWS Documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_billing.html))
  - **Bedrock** : tokens moyens par requ√™te (input/output), mod√®le(s), proportion batch vs realtime. ([Amazon Web Services, Inc.](https://aws.amazon.com/bedrock/pricing/))
  
  ------
  
  ### 5.2.11 Optimisations budg√©taires recommand√©es
  
  L‚ÄôOPEX d‚Äôun MVP RAG/Chatbot (AWS + Bedrock + Zilliz + Fargate) est domin√© par 3 postes :
   **(1) co√ªts LLM/tokens**, 
  **(2) compute (serving + jobs batch)**, 
  **(3) stockage/observabilit√©**. Les optimisations ci-dessous visent √† **r√©duire le co√ªt par requ√™te**, **stabiliser la latence**, et **√©viter les explosions de facture** tout en gardant la qualit√© produit (pertinence + fiabilit√©).
  
  ------
  
  ## 5.2.11.1 Optimiser le co√ªt LLM : r√©duire tokens, mieux choisir quand appeler un ‚Äúgros‚Äù mod√®le
  
  Le levier le plus rentable est presque toujours de **diminuer le nombre de tokens** et de **r√©duire la fr√©quence d‚Äôappel** aux mod√®les co√ªteux.
  
  ### 1.1 Model routing (multi-mod√®le) : ‚Äúle bon mod√®le au bon moment‚Äù
  
  Mettre en place une strat√©gie ‚Äúrouter‚Äù :
  
  - **Petit mod√®le** (peu cher, rapide) pour :
    - classification d‚Äôintention (recherche / comparaison / Q&A / r√©servation / support)
    - extraction de contraintes structur√©es (ville, dates, rayon, cat√©gories, online/offline)
    - reformulation de requ√™te (query rewrite l√©ger)
  - **Grand mod√®le** (plus cher) uniquement pour :
    - g√©n√©ration finale lorsque la requ√™te est complexe
    - synth√®se multi-√©v√©nements, justification, r√©ponses longues
  - **Fallback** :
    - si budget journalier d√©pass√© ‚Üí bascule sur mod√®le plus petit + r√©ponse plus concise
    - si latence trop √©lev√©e ‚Üí mode ‚Äúfast response‚Äù
  
  B√©n√©fices :
  
  - baisse imm√©diate de la facture tokens
  - meilleure latence p95
  - capacit√© √† ‚Äútenir‚Äù les pics
  
  ### 1.2 R√©duction tokens (prompt & contexte) : compacter sans perdre la qualit√©
  
  Actions concr√®tes :
  
  - **Prompts courts et stables** : templates minimalistes, suppression des redondances, consignes compactes.
  - **Contexte RAG minimal** :
    - limiter `top-k` (ex. 5‚Äì8) et augmenter seulement si retrieval pauvre
    - limiter la longueur de chaque chunk (ou ‚Äúevent card‚Äù) inject√© au mod√®le
    - pr√©f√©rer des ‚Äúsnippets‚Äù + URLs plut√¥t que des descriptions longues
  - **Compression historique / m√©moire** :
    - au lieu de r√©injecter tout l‚Äôhistorique, utiliser :
      - une **fen√™tre glissante** (N derniers tours)
      - un **r√©sum√© de session** (1 paragraphe)
      - des **slots structur√©s** (ville=‚Ä¶, rayon=‚Ä¶, dates=‚Ä¶, th√®mes=‚Ä¶)
    - cela r√©duit drastiquement les tokens ‚Äúinutilement‚Äù envoy√©s √† chaque tour.

  Pourquoi c‚Äôest essentiel : Bedrock facture l‚Äôinf√©rence selon **tokens input + output** et propose des options/tier et du batch selon mod√®les ‚Äî donc tout ce qui diminue le volume de tokens diminue m√©caniquement le co√ªt.
  
  ### 1.3 Contr√¥ler la longueur de sortie (output tokens)
  
  M√™me si on optimise l‚Äôinput, la facture peut exploser sur l‚Äôoutput si le mod√®le ‚Äúparle trop‚Äù.
  
  - imposer une **r√©ponse structur√©e** et **born√©e** (ex. 5 √©v√©nements max, 2 phrases de justification max par √©v√©nement)
  - activer un mode ‚Äúr√©sum√© court‚Äù par d√©faut, avec ‚ÄúSouhaites-tu plus de d√©tails ?‚Äù
  - √©viter les longues digressions en for√ßant un format (JSON + rendu UI).
  
  ### 1.4 Batch quand c‚Äôest possible (offline plut√¥t que realtime)
  
  Tout ce qui n‚Äôa pas besoin d‚Äô√™tre instantan√© doit √™tre ‚Äúbatch√©‚Äù :
  
  - r√©sum√©s long-terme, consolidation m√©moire
  - enrichissements d‚Äô√©v√©nements
  - recalculs d‚Äôembeddings (re-indexation)
  - √©valuations offline (scoring, tests)
  
  Bedrock pr√©cise que le pricing d√©pend tokens et options (incluant ‚ÄúProvisioned Throughput‚Äù selon mod√®les) et que certaines approches (ex. batch) peuvent r√©duire les co√ªts lorsqu‚Äôelles sont adapt√©es au besoin.

  ------

  ## 5.2.11.2 Optimiser le retrieval pour limiter le contexte (et donc les tokens) sans perdre la pertinence

  Dans un RAG, un retrieval ‚Äúbrouillon‚Äù co√ªte cher : il force √† envoyer beaucoup de texte au mod√®le ou √† rater la r√©ponse.

  ### 2.1 Retrieval efficace : index + filtres + rerank cibl√©
  
  - **Filtres metadata** (ville, dates, cat√©gorie, status, online/offline) pour r√©duire l‚Äôespace de recherche avant vector search.
  - **Rerank l√©ger** (MMR ou r√®gles) apr√®s le top-k pour √©viter d‚Äôenvoyer des doublons (m√™mes √©v√©nements/lieux).
  - Ajuster `top-k` dynamiquement :
    - si la requ√™te est tr√®s contrainte (‚Äúce soir √† Lyon jazz gratuit‚Äù) ‚Üí `top-k` faible
    - si la requ√™te est vague (‚Äúdes id√©es de sortie‚Äù) ‚Üí `top-k` un peu plus haut mais toujours born√©.
  
  R√©sultat : moins de ‚Äúpassages‚Äù √† injecter ‚Üí moins de tokens ‚Üí moins de co√ªt et souvent meilleure latence.

  ### 2.2 Semantic caching (avec pr√©cautions multi-tours)
  
  Le **semantic cache** (cache par similarit√©) peut r√©duire drastiquement les co√ªts sur les questions r√©p√©titives :
  
  - ‚ÄúQue faire ce soir pr√®s de moi ?‚Äù
  - ‚ÄúDes expos gratuites ce week-end ?‚Äù
  - ‚ÄúDonne-moi 5 id√©es de sorties √† Paris‚Äù

  Mais attention :
  
  - en multi-tours, deux requ√™tes ‚Äúsemblables‚Äù peuvent d√©pendre d‚Äôun contexte diff√©rent (ville/rayon/date)
  - il faut inclure les **slots** (ville/date/rayon/langue) dans la cl√© de cache
  - TTL court (ex. 1h‚Äì24h) pour rester √† jour (√©v√©nements changent).
  
  ------
  
  ## 5.2.11.3 Optimiser le compute : Fargate, Spot, et bonnes pratiques d‚Äôautoscaling
  
  Le compute se divise en 2 familles : **serving** (API/chat) et **batch** (ingestion, embeddings, reindex).
  
  ### 3.1 Fargate Spot pour les workloads interruptibles (gains majeurs)
  
  Pour tout ce qui peut √™tre relanc√© sans impact utilisateur :
  
  - jobs d‚Äôingestion
  - ETL/normalisation
  - embeddings & reindex
  - √©valuations offline
  
  Fargate Spot peut offrir jusqu‚Äô√† **~70%** de r√©duction selon AWS (et d√©pend de la disponibilit√©).
  
  ### 3.2 Savings Plans pour les services 24/7 (serving stable)
  
  Si l‚ÄôAPI tourne en permanence, les **Compute Savings Plans** r√©duisent le co√ªt quand on a un socle de consommation stable (toujours selon conditions AWS).
  
  ### 3.3 Autoscaling ‚Äúpropre‚Äù : √©viter le surprovisionnement permanent
  
  - dimensionner une ‚Äúbase‚Äù minimale (1‚Äì2 tasks) + scale sur CPU/RAM + latence
  - limiter la m√©moire pour √©viter over-allocation
  - s√©parer ‚ÄúAPI‚Äù et ‚Äúworkers‚Äù pour ne pas scaler tout le cluster √† cause d‚Äôun batch.
  
  ------
  
  ## 5.2.11.4 Optimiser l‚Äôobservabilit√© : mesurer assez, sans payer trop
  
  L‚Äôobservabilit√© est critique en GenAI (debug + qualit√©), mais peut co√ªter tr√®s cher (logs + traces).
  
  ### 4.1 Sampling intelligent (traces)
  
  - **Stage** : 100% des traces (debug/qualit√©)
  - **Prod** : 5‚Äì20% selon stabilit√© + ‚Äúburst to 100%‚Äù pendant incident
  - garder les traces compl√®tes pour les requ√™tes ‚Äú√† probl√®me‚Äù (erreurs, low-quality, thumbs down)
  
  CloudWatch facture sur logs/metrics/alarms ; limiter les m√©triques custom et contr√¥ler la r√©tention √©vite l‚Äôexplosion de co√ªt.
  
  ### 4.2 Logs : r√©tention courte + export S3
  
  - r√©tention CloudWatch courte (ex. 7‚Äì14 jours)
  - export des logs ‚Äúfroids‚Äù dans S3 (moins cher, audit possible)
  - filtrer/scrubber PII avant ingestion log.
  
  ### 4.3 Limiter les ‚Äúcustom metrics‚Äù
  
  - cr√©er peu de m√©triques custom mais tr√®s utiles (p95 latence, tokens, co√ªt estim√©/req, satisfaction)
  - √©viter d‚Äô√©mettre une m√©trique par type de requ√™te/ville/tag (cardinalit√© √©norme).
  
  ------
  
  ## 5.2.11.5 Optimiser Zilliz : choisir le bon mode et √©viter de payer pour de l‚Äôindex inutile
  
  La vector DB peut √™tre tr√®s √©conomique au d√©but, mais grimper vite si :
  
  - index trop volumineux,
  - r√©indexations fr√©quentes,
  - top-k trop √©lev√©, ou
  - metadata non ma√Ætris√©es.
  
  ### 5.1 D√©marrer Serverless, basculer Dedicated selon SLO
  
  - **Serverless** : id√©al si charge variable, MVP early
  - **Dedicated** : quand vous avez des objectifs de latence/throughput plus stricts, et des charges pr√©visibles
  - **Marketplace AWS** : utile si besoin procurement, facturation centralis√©e, gouvernance FinOps.
  
  Zilliz met en avant Serverless et Dedicated (‚Äúfrom $99/GB/month‚Äù) sur sa page pricing.
  
  ### 5.2 R√©duire la taille index√©e
  
  - n‚Äôembedder que ce qui sert (titre + r√©sum√© + √©l√©ments discriminants)
  - √©viter d‚Äôembedder des champs longs et redondants (ex. descriptions tr√®s longues) si pas utiles
  - g√©rer les mises √† jour incremental plut√¥t que reindex complet.
  
  ------
  
  ## 5.2.11.6 ‚ÄúPack FinOps MVP‚Äù : garde-fous indispensables (anti-d√©rapage)
  
  √Ä int√©grer d√®s le MVP (sinon la facture devient le ‚Äúbug #1‚Äù) :
  
  - **budget mensuel** (AWS Budgets) + alertes
  - ‚Äúhard limits‚Äù applicatifs :
    - max tokens / r√©ponse
    - max tool calls (web search)
    - max top-k retrieval
    - quota par utilisateur (free vs premium)
  - dashboard ‚Äúco√ªt par requ√™te‚Äù + ‚Äúco√ªt par utilisateur actif‚Äù
  - mode d√©grad√© : si budget atteint ‚Üí r√©ponses plus courtes + mod√®le plus petit + web search off.
  
  ------
  
  ## 6. Bilan
  
  ### 6.1 Rappel des √©tapes cl√©s du POC au MVP
  
  - POC : d√©monstration faisabilit√© (semantic search + chatbot RAG)
  - MVP : industrialisation via :
    - pipelines data robustes (ingestion + DQ)
    - retrieval hybride + g√©o
    - m√©moire multi-sessions (STM/LTM + checkpointing)
    - garde-fous (structure + policies)
    - observabilit√© + √©valuation continue
  
  ### 6.2 Justification des choix techniques et m√©thodologiques
  
  - **M√©moire** : indispensable pour personnalisation ; n√©cessite gestion stockage/retrieval + limitation de contexte + checkpointing.
  - **RAG modulaire** : s√©paration en pipelines pour scaler et travailler en parall√®le.
  - **Index/traceability** : exigence produit (‚Äúpourquoi ?‚Äù) et debug ; la tra√ßabilit√© est un avantage majeur des approches index√©es.
  - **AWS Bedrock** : s√©curit√©/gouvernance/metrics int√©gr√©es (CloudWatch, CloudTrail, VPC endpoints).
  - **Backlog agile ordonn√© + refinement** : pr√©paration continue, √©viter stories ‚Äútrop grosses‚Äù, d√©couper par valeur d√©montrable.
  
  ### 6.3 D√©fis rencontr√©s (attendus) & solutions
  
  - **Hallucinations / incoh√©rences** ‚Üí guardrails + citations + eval set + feedback
  - **Latence** ‚Üí cache + topK limit√© + reranking cibl√©
  - **Qualit√© data** ‚Üí DQ checks + versioning raw + monitoring anomalies
  - **PII / conformit√©** ‚Üí minimisation des champs contributeurs, masquage, consentement
  - **Co√ªts tokens** ‚Üí routing + r√©duction prompt + m√©moire r√©sum√©e
  
  ------
  
  ## Conclusion
  
  Ce design transforme le POC en un MVP **d√©ployable, mesurable et it√©ratif**, en adressant explicitement :
  
  - la **m√©moire** (STM/LTM + checkpointing),
  - le **g√©ographique** (PostGIS + ranking),
  - le **temps r√©el** (web tool + ingestion fr√©quente),
  - le **monitoring/LLMOps** (CloudWatch/CloudTrail + LangSmith + Phoenix + feedback).